---
layout: post
title: The curse of dimensionality and the weak law of large numbers
image:
  feature: sample-image-22.jpg
  credit: Royal National Park, New South Wales, 2014
---

I was recently re-reading some fantastic course [notes](https://terrytao.wordpress.com/2015/10/23/275a-notes-3-the-weak-and-strong-law-of-large-numbers/) by the peerless Terence Tao on basic statistics from a fairly high mathematical standpoint (wanted a quick refresher on a couple of things), and came across a very nice connection between two important and beautiful results which are fundamental to statistics and machine learning.

## The weak law of large numbers

The weak law of large numbers is an important theorem in statistics which underpins a lot of analysis and model-building, so much that most data scientists probably invoke it multiple times a day without thinking. It tells us that the sample mean of independently, identitically distributed random variables converges in probability to the expected value, with very weak assumptions on the random variable itself. This allows us to "assume normality" in a large number of cases, and lets us use the power of statistical tests based on the normal distribution in a large number of contexts.

We can state the weak law of large numbers more formally as follows. Suppose we are given a sequence $X_1, X_2, \ldots$ of [iid](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables) copies of a random variable $X$ with expected value $\mathbb{E}X=\mu$. If we denote by $Y_n$ the sum $X_1+X_2+\ldots+X_n$, then the random variables $Y_n/n$ converge in probability to $\mu$.

The weak law of large numbers is of enormous consolation to casino operators. Indeed, suppose that the random variable $X$ models the result of spinning a roulette wheel. Then at any given $X_i$, there is a chance of a significant loss; but over time we are guaranteed that the total loss per spin will converge to some $\mu$, and we can calibrate the payouts accordingly so our bottom line is positive.

## The curse of dimensionality

The curse of dimensionality is a catch-all used by statisticians and mathematicians to describe a bunch of highly counterintuitive behaviours which occur at "non-spatial" dimensions (i.e. $n>3$).

The most well-known, and most prone to cause issues, is the strange density property of high-dimensional space: in a very vague sense, paraphrasing Wikipedia, there is no space in the "middle" of a unit hypercube; and it is almost all "pushed" out to the corners.

We can use the weak law of large numbers elegantly to make this notion precise.

Choose $1 > \epsilon > 0$. Then for $n$ sufficiently large, at least $100(1-\epsilon)\%$ of the unit hypercube $[-1,1]^n$ is contained in the set 

$$
\{x\in \mathbb{R}^n\mid (1-\epsilon)\sqrt{n/3}\leq \left|x\right| \leq (1+\epsilon)\sqrt{n/3}\}.
$$

## Proof

Following Terence Tao's notes, we let $X_1,X_2,\ldots$ be drawn uniformly from the interval $[-1,1]$ which gives 
a vector $(X_1,X_2,\ldots,X_n)$ uniformally distributed on $[-1,1]^n$.
Their squares $X_1^2,X_2^2,\ldots$ are also iid random variables and we
can compute their expectations as

$$
\int_0^1x^2dx = 1/3.
$$

The weak law of large numbers tells us that the quantity $\frac{X_1^2+\ldots+X_n^2}{n}$ converges in probability to $\frac13$, which means that for a given epsilon we can find $n$ such that

$$
P(\frac{X_1^2+\ldots+X_n^2}{n} - \frac13) < \epsilon,
$$

i.e. 

$$
P(X_1^2+\ldots+X_n^2 - \frac{n}{3}) < \epsilon,
$$
and taking square roots of both sides we see this is precisely the definition
of $(X_1,\ldots,X_n)$ lying in the region described above. 


